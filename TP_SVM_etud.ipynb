{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP I : Support Vector Machines\n",
    "\n",
    "Ce TP vise à apporter les éléments nécessaires pour comprendre les implementations d'algorithmes de classification autour des SVM. **C'est un *TP à trous* ; il s'agira de compléter ces trous et d'y ajouter les tests qui vous sembleront utiles.**\n",
    "\n",
    "Voici un aperçu des points abordés lors de ce TP.\n",
    "\n",
    "- partie I\n",
    "    - Importation et visualisation des données\n",
    "- partie II\n",
    "    - SVM Linéaire\n",
    "- partie III\n",
    "    - SVM non-Linéaire\n",
    "    \n",
    "Dans l'ensemble du déroulé du TP vous ferez bien attention à valider par un jeu de tests la validité des programmes écrits. Vous regarderez l'influence des paramètres sur la convergence. Vous comparerez les intérêts des diférentes méthodes les unes par rapport aux autres.\n",
    "\n",
    "**Votre démarche et vos conclusions seront présentées sous forme d'un rapport (de 5 à 10 pages) où vous pourrez choisir d'avoir une approche plus sur les résultats mathématiques ou sur les résultats numériques. Le notebook Jupiter avec l'ensemble de vos travaux sera aussi remis. Ces deux éléments devront être remis au plus tard le mercredi 28 décembre.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attendus de rendu\n",
    "\n",
    "Votre rendu sera jugé à l'aune de\n",
    "\n",
    "- votre capacité à produire des algorithmes valides, répondant à la question posée\n",
    "- l'étude effectuée concernant la sensibilité de vos algorithmes aux hyperparamètres / conditions initiales\n",
    "- l'analyse comparative proposée quant aux différentes implémentations suggérées \n",
    "- les stress-tests auxquels vous aurez confrontés vos implémentations. \n",
    "\n",
    "On portera une attention particulière à la *généricité* de votre réponse ; tout comme cela vous est suggéré par la suite on attendra de vous d'appuyer vos affirmations par suffisamment de tests et une appréciation pour les limites de votre analyse.\n",
    "\n",
    "Ce TP est à rendre par **groupes de 3** et exceptionnellement **2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Au travail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Importation et préparation des données\n",
    "\n",
    "Nous allons ici introduire quelques datasets qui permettront de tester les méthodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Importer **data1** le dataset proposé pour faire les premiers tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('data1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Identifier la colonne qui servira à classifier et qui ne prend donc que 2 valeurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualiser les deux premières colonnes qui ont des valeurs numériques, sous forme d'un nuage de points (avec un marqueur différent en fonction de la valeur servant à classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu(df_):\n",
    "    value=(df_['classe']>0)\n",
    "    df_['color']= np.where( value==True , \"red\", \"blue\")\n",
    "\n",
    "    plt.scatter(df_.iloc[:,1],df_.iloc[:,2],color= df_.color)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    df1.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarder aussi les fichiers **data2**, **data3** et **data4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- SVM - le cas linéaire\n",
    "\n",
    "Nous allons ici rappeler les points essentiels qui ont été vus en cours et qui serviront à l'implémentation de l'algorithme.\n",
    "\n",
    "On a une population de $n$ individus.\n",
    "\n",
    "On dispose, pour chaque individu $i$, de ses caractéristiques $X_i \\in \\mathbb{R}^p$ et d'un indicateur de décision $y_i \\in \\{ -1 ; +1\\}$.\n",
    "\n",
    "Le problème de SVM linéaire revient à chercher $(\\omega,b)\\in \\mathbb{R}^p \\times \\mathbb{R}$ tel que $(\\omega,b)$ soit solution du problème d'optimisation sous contraintes:\n",
    "\n",
    "$\\displaystyle (HM):\\quad \\min_{\\begin{matrix} \\omega,b \\, t.q. \\\\ y_i\\left( \\omega . X_i  - b\\right) \\ge 1 \\\\ 1\\le i \\le N \\end{matrix}}\\frac{1}{2}||\\omega||^2$.\n",
    "\n",
    "Géométriquement cela revient à séparer les points $X_i$ par des hyperplans:\n",
    "- $\\omega . X  - b = 0$ qui sera l'hyperplan de décision.\n",
    "- Les deux hyperplans $\\omega . X  - b = +1$ et $\\omega . X  - b = -1 $ étant espacés au maximum tout en gardant les points tels que $y_i=+1$ et $y_i=-1$ à l'extérieur de la bande qu'ils délimitent.\n",
    "\n",
    "Le problème ci-dessus est le **problème primal**. Nous avons vu en cours sur l'optimisation sous contrainte (et après introduction du Lagrangien), que ce problème est équivalent au **problème dual**\n",
    "\n",
    "$\\displaystyle (HM*):\\quad \\min_{\\begin{matrix} \\alpha_i \\ge 0 \\\\ \\sum \\alpha_i y_i =0 \\end{matrix}}\\frac{1}{2}\\sum_{1\\le i,j \\le N} \\alpha_i \\alpha_j y_i y_j X_i^TX_j-\\sum_{1\\le i \\le N}\\alpha_i$.\n",
    "\n",
    "où les inconnues sont les $(\\alpha_1,\\cdots, \\alpha_n)$ qui sont les multiplicateurs de Lagrange.\n",
    "\n",
    "Si on note $G=\\big(y_i y_j X_i ^T X_j \\big)_{\\begin{matrix} 1 \\le i \\le N \\\\ j \\le i \\le N \\end{matrix}}$, la matrice de Gram, le problème s'écrit:\n",
    "\n",
    "$\\displaystyle (HM*):\\quad \\min_{\\begin{matrix} \\alpha_i \\ge 0 \\\\ \\sum \\alpha_i y_i =0 \\end{matrix}}\\frac{1}{2} \\alpha^TG\\alpha-\\sum_{1\\le i \\le N}\\alpha_i$.\n",
    "\n",
    "Ce problème peut se résoudre en utilisant par exemple un algorithme de gradient projeté car $\\displaystyle \\{ \\alpha \\in \\mathbb{R}^N \\, t.q. \\, \\alpha_i \\ge 0 \\, , \\, 1 \\le i \\le N  \\, et \\, \\sum_{i=0}^n \\alpha_i y_i =0 \\}$ est un convexe sur lequel il est \"relativement\" facile de projeter un vecteur de $ \\mathbb{R}^N$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Une fois les $\\alpha=(\\alpha_1,\\cdots,\\alpha_N)$ calculés, on se souvient que comme on est situé au niveau du point selle du Lagrangien, on a aussi $\\displaystyle \\omega = \\sum_{1 \\le i \\le N}\\alpha_i yi X_i$. Cela nous permet de calculer $\\omega$\n",
    "\n",
    "On remarque que beaucoup de $\\alpha_i$ sont nuls, ce qui signifient que ces contraintes ne sont pas qualifiées (au sens de la théorie du Lagrangien). Les $i$ qui correspondent à des $\\alpha_i \\neq 0$ correspondent aux Support Vectors (c'est à dire aux $X_i$ situés sur les hyperplans définissant la marge) et donc $F(X_i) = \\pm 1$ pour ces vecteurs.\n",
    "\n",
    "On pourra se servir de la remarque ci-dessus pour calculer $b$.\n",
    "\n",
    "On aura donc ainsi entièrement défini notre hyperplan qui constitue la \"decision boundary\".\n",
    "\n",
    "**On peut aussi voir les choses un peu différemment (et cela sera utile plus tard pour traiter le cas non linéaire):**\n",
    "- En introduisant la fonction $\\displaystyle F(X) = \\sum_{1 \\le i \\le N}\\alpha_i yi X_i^T X - b$ qui va de $\\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R}$.\n",
    "\n",
    "- On a $C_0 F$ qui est l'hyperplan de séparation entre les points $+1$ et les points $-1$. et $C_{-1} F$ et $C_{+1} F$ sont les deux hyperplans de marge (on rappelle que $C_k F$ est la courbe de niveau de valeur $k$ pour la fonction $F$, c'est à dire $\\{ X \\in \\mathbb{R}^m \\, t.q. F(X)=k \\}$ ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Le gradient projeté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Créer un algorithme de descente de gradient (par exemple récupéré dans le TP de descente de gradient de OCVX1).\n",
    "Le modifier pour qu'à chaque pas de descente, le gradient soit projeté sur $\\{ \\alpha \\in \\mathbb{R}^N \\, t.q. \\, \\alpha_i \\ge 0 \\, , \\, 1 \\le i \\le N  \\, et \\, \\sum_{i=0}^n \\alpha_i y_i =0 \\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(f, x, i=0, dx=1e-6):\n",
    "    \"\"\"Computes i-th partial derivative of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which partial derivative is computed.\n",
    "        i: coordinate along which derivative is computed.\n",
    "        dx: slack for finite difference.\n",
    "        \n",
    "    Output:\n",
    "        (float)\n",
    "\n",
    "    \"\"\"\n",
    "    h = np.zeros(x.size)\n",
    "    h[i] = dx\n",
    "    return (f(x + h) - f(x - h)) / (2*dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, x, dx=1e-6):\n",
    "    \"\"\"Computes gradient of f at point x.\n",
    "    \n",
    "    Args:\n",
    "        f: objective function.\n",
    "        x: point at which gradient is computed.\n",
    "        dx: slack for finite difference of partial derivatives.\n",
    "        \n",
    "    Output:\n",
    "        (ndarray) of size domain of f.\n",
    "        \n",
    "    \"\"\"\n",
    "    dim=x.size\n",
    "    grad=np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        grad[i]=partial(f,x,i,dx)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction génère la taille du pas optimal vérifiant le critère d'Amijo\n",
    "\n",
    "def backtrack(x0, f , dir_x, alpha = 0.4, beta = 0.8):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Parameters:\n",
    "    x0: point actuel\n",
    "    f: fonction à minimiser\n",
    "    dir_x: direction dans laquelle on souhaite aller\n",
    "    \n",
    "    Output\n",
    "    valeur du pas optimal\n",
    "    \"\"\"\n",
    "    t=1\n",
    "    grad=gradient(f,x0)\n",
    "\n",
    "#on teste qu'on est bien dans une direction de descente\n",
    "    if dir_x.T @ grad < 0:        \n",
    "        while True:\n",
    "            if f(x0 + t*dir_x) < f(x0) + alpha * t * dir_x.T @ grad:\n",
    "                break\n",
    "            t =t * beta\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**C'est la fonction ci-dessous qu'il faut modifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_grad_opti_proj (f,y,p0,eps=1E-5):\n",
    "    x=[np.array(p0)]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        descente = - gradient(f,x[-1])\n",
    "        mu = backtrack(x[-1], f, descente)\n",
    "        x.append(x[-1] + mu * descente)\n",
    "#     modifier la ligne ci-dessus pour projeter\n",
    "#     sur le convexe R+^n et le plan somme(alpha_i yi)=0 \n",
    "        \n",
    "        # test de convergence\n",
    "        if np.square(x[-1]-x[-2]).sum() < eps**2 :\n",
    "            break\n",
    "        # tests de protection\n",
    "        if np.square(x[-1] - x[-2]).sum() > 1E9:  # au cas où on diverge\n",
    "            print(\"DIVERGE\")\n",
    "            break\n",
    "        if len(x) > 30000:  # c'est trop long, je crains la boucle infinie\n",
    "            print('Trop long, boucle infinie ?')\n",
    "            break\n",
    "    return np.array(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Résolution du problème dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Créer une fonction prepa_donnees qui prépare les données dont on a besoin (df,X,y,Gram) pour la résolution du problème dual à partir d'un fichier \"nom_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepa_donnees(nom_data):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Parameters:\n",
    "    nom_data: nom du fichier dans lequel aller chercher les données\n",
    "    \n",
    "    Output\n",
    "    df: dataframe panda contenant les données\n",
    "    X: un tableau numpy de taille (nb_individus x nb_caractéristiques) contenant les caractéristiques\n",
    "    y: un vecteur numpy de taille (nb_individus) contenant les classificateurs (+1 ou -1)\n",
    "    Gram: la matrice de Gram du problème de taille (nb_individus x nb_individus)\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    return df,X,y,Gram\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Définir une fonction qui définit la fonctionnelle à minimiser $\\displaystyle J(\\alpha)=  \\frac{1}{2} \\alpha^TG\\alpha-\\sum_{1\\le i \\le N}\\alpha_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def J(alpha):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Avec les modules que vous venez de créer, vous pouvez maintenant calculer les valeurs de $\\alpha_1,\\cdots,\\alpha_N$ comme étant la solution du problème $(HM*)$. Commencez avec le fichier **data1**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculer maintenant $\\omega$ et $b$. Ici on aura $\\omega = (a_1,a_2)$ et notre hyperplan se limitera à la droite $a_1 x + a_2 y +b=0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualisez les résultats en reprenant la visualisation du nuage de nos points réalisé au $\\bf{I-2}$ et en rajoutant les hyperplans caractéristiques qu'on vient de calculer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A ce stade ... si tout se passe bien vous avez dû trouver, en utilisant les données de **data1**, que la frontière de décision est la droite $y=x+1$ et que les deux frontières pour la marge maximum sont les droites  $y=x$ et $y=x+2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_front(df,a1_,a2_,b_):\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_front(df,a1,a2,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cas plus large et cas non séparable linéairement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tester ce que vous venez de faire sur **data2** et **data3**.\n",
    "Que remarquez vous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- SVM - le cas non linéaire - \"the Kernel Trick\"\n",
    "\n",
    "Comme nous avons vu en TD, on peut chercher à séparer nos points avec des fonctions plus compliquées que des hyperplans en se projetant dans un espace de dimension plus grande.\n",
    "\n",
    "Ce qui permet de faire cela sans que cela représente un coût trop élevé est la remarque que cette projection dans un espace de dimension plus grande n'a pas besoin d'être explicitée. On n'aura besoin que d'une expression des produits scalaires des projetés.\n",
    "\n",
    "En pratique on se donne donc un noyau $K: \\mathbb{R}^N \\times \\mathbb{R}^N \\to \\mathbb{R}$ de la forme $K(q,q')$ qui possède la propriété d'être symétrique et défini positif.\n",
    "\n",
    "On est ramené au même propblème dual que précédemment en remplaçant les produits scalaires par ce noyau.\n",
    "\n",
    "Le problème (dual) à résoudre est donc:\n",
    "\n",
    "$\\displaystyle (HM**):\\quad \\min_{\\begin{matrix} \\alpha_i \\ge 0 \\\\ \\sum \\alpha_i y_i =0 \\end{matrix}}\\frac{1}{2}\\sum_{1\\le i,j \\le N} \\alpha_i \\alpha_j y_i y_j K(X_i,X_j)-\\sum_{1\\le i \\le N}\\alpha_i$.\n",
    "\n",
    "où les inconnues sont les $(\\alpha_1,\\cdots, \\alpha_n)$ qui sont les multiplicateurs de Lagrange.\n",
    "\n",
    "Si on note $G=\\big(y_i y_j K(X_i,X_j) \\big)_{\\begin{matrix} 1 \\le i \\le N \\\\ j \\le i \\le N \\end{matrix}}$, la matrice de Gram, le problème s'écrit:\n",
    "\n",
    "$\\displaystyle (HM**):\\quad \\min_{\\begin{matrix} \\alpha_i \\ge 0 \\\\ \\sum \\alpha_i y_i =0 \\end{matrix}}\\frac{1}{2} \\alpha^TG\\alpha-\\sum_{1\\le i \\le N}\\alpha_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Une fois les $\\alpha=(\\alpha_1,\\cdots,\\alpha_N)$ calculés, on introduit la fonction $\\displaystyle F(X) = \\sum_{1 \\le i \\le N}\\alpha_i yi K(X_i, X)$ qui va de $\\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R}$.\n",
    "\n",
    "On a $C_0 F$ qui est l'hyperplan de séparation entre les points $+1$ et les points $-1$. et $C_{-1} F$ et $C_{+1} F$ sont les deux hyperplans de marge (on rappelle que $C_k F$ est la courbe de niveau de valeur $k$ pour la fonction $F$, c'est à dire $\\{ X \\in \\mathbb{R}^m \\, t.q. F(X)=k \\}$ ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduire la méthode du Kernel et tester sur un cas simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Définir le noyau polynomial $K(q,q')=(1+q.q')^d$. Prendre d=2 pour commencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(q1,q2,d=6):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Modifier la fonction prepa_donnees qui prépare les données dont on a besoin (df,X,y,Gram) pour la résolution du problème dual à partir d'un fichier \"nom_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepa_donnees_ker(nom_data):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Parameters:\n",
    "    nom_data: nom du fichier dans lequel aller chercher les données\n",
    "    \n",
    "    Output\n",
    "    df: dataframe panda contenant les données\n",
    "    X: un tableau numpy de taille (nb_individus x nb_caractéristiques) contenant les \n",
    "    y: un vecteur numpy de taille (nb_individus) contenant les classificateurs (+1 ou -1)\n",
    "    Gram: la matrice Gram du problème de taille (nb_individus x nb_individus)\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    return df,X,y,Gram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Avec les modules que vous venez de créer, vous pouvez maintenant calculer les valeurs de $\\alpha_1,\\cdots,\\alpha_N$ comme étant la solution du problème $(HM**)$ avec le fichier **data3**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Introduire la nouvelle fonction \"séparatrice\" $\\displaystyle F(X) = \\sum_{1 \\le i \\le N}\\alpha_i yi K(X_i, X) - b$ qui va de $\\mathbb{R}^m \\times \\mathbb{R}^m \\to \\mathbb{R}$.\n",
    "\n",
    "$b$ sera calculé en reprenant la remarque que $F(X_i)=\\pm 1$ quand $X_i$ correspond à un Support Vector et donc que $\\alpha_i \\ne 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_separ(Z):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualisez les résultats prenant le fichier **data3** avec le nuage de points et en rajoutant les courbes de niveau $-1$, $0$ et $+1$ de la fonction F_separ que nous venons d'introduire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_front_ker(df,X):\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_front_ker(df,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cas plus large et cas non séparable linéairement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tester ce que vous venez de faire sur **data4**.\n",
    "Que remarquez vous? Comment améliorer la situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
